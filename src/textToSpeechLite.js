import { KokoroTTS } from 'kokoro-js';
import { config } from './config.js';
import { env } from '@huggingface/transformers';
import { VOICES } from './voices.js';

// å¯ç”¨æµè§ˆå™¨ç¼“å­˜ä¼˜å…ˆç­–ç•¥ï¼ˆä¼˜å…ˆä½¿ç”¨ Cache API å’Œ Service Workerï¼‰
env.useBrowserCache = true;
env.useFSCache = true;

// é…ç½®é•œåƒæœåŠ¡å™¨ï¼ˆå¿…é¡»åœ¨å…¨å±€ env è®¾ç½®ï¼‰
if (config.mirror.enabled && config.mirror.url) {
    env.remoteHost = config.mirror.url;
    env.remotePathTemplate = '{model}/resolve/{revision}/';
    console.warn(`[TextToSpeechLite] âœ… Global mirror configured: ${config.mirror.url}`);
}



/**
 * åˆå¹¶å¤šä¸ª WAV Blob æ–‡ä»¶
 * @param {Blob[]} blobs - è¦åˆå¹¶çš„ WAV Blob æ•°ç»„
 * @returns {Promise<Blob>} åˆå¹¶åçš„ WAV Blob
 */
async function mergeWavBlobs(blobs) {
    if (blobs.length === 0) return null;
    if (blobs.length === 1) return blobs[0];

    console.warn(`[WAV Merge] Merging ${blobs.length} WAV files...`);

    // è¯»å–æ‰€æœ‰ WAV æ–‡ä»¶çš„ ArrayBuffer
    const buffers = await Promise.all(blobs.map(blob => blob.arrayBuffer()));

    // è§£æç¬¬ä¸€ä¸ª WAV æ–‡ä»¶çš„å¤´éƒ¨ä¿¡æ¯
    const firstBuffer = buffers[0];
    const view = new DataView(firstBuffer);

    // æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ WAV æ–‡ä»¶
    const riff = String.fromCharCode(view.getUint8(0), view.getUint8(1), view.getUint8(2), view.getUint8(3));
    if (riff !== 'RIFF') {
        console.error('[WAV Merge] Invalid WAV file: RIFF header not found');
        return blobs[0];
    }

    // è¯»å–éŸ³é¢‘å‚æ•°
    const channels = view.getUint16(22, true);
    const sampleRate = view.getUint32(24, true);
    const bitsPerSample = view.getUint16(34, true);

    console.warn(`[WAV Merge] Audio format: ${channels} channels, ${sampleRate}Hz, ${bitsPerSample}bit`);

    // æ£€æŸ¥éŸ³é¢‘æ ¼å¼
    const audioFormat = view.getUint16(20, true);
    const isFloat = audioFormat === 3; // 3 = IEEE float
    console.warn(`[WAV Merge] Audio format code: ${audioFormat} (${isFloat ? 'IEEE Float' : 'PCM'})`);

    // æå–æ‰€æœ‰æ–‡ä»¶çš„éŸ³é¢‘æ•°æ®ï¼ˆè·³è¿‡ WAV å¤´éƒ¨ï¼Œæ ‡å‡† WAV å¤´éƒ¨æ˜¯ 44 å­—èŠ‚ï¼‰
    const audioDataArrays = buffers.map((buffer, index) => {
        // æ ‡å‡† WAV æ–‡ä»¶å¤´éƒ¨æ˜¯ 44 å­—èŠ‚
        const headerSize = 44;
        const dataSize = buffer.byteLength - headerSize;
        const audioData = new Uint8Array(buffer, headerSize, dataSize);
        console.warn(`[WAV Merge] File ${index + 1}: ${dataSize} bytes of audio data`);
        return audioData;
    });

    // è®¡ç®—åˆå¹¶åçš„æ€»å¤§å°
    const totalDataSize = audioDataArrays.reduce((sum, arr) => sum + arr.length, 0);
    console.warn(`[WAV Merge] Total audio data: ${totalDataSize} bytes`);

    // åˆ›å»ºæ–°çš„ WAV æ–‡ä»¶
    const wavBuffer = new ArrayBuffer(44 + totalDataSize);
    const wavView = new DataView(wavBuffer);
    const wavBytes = new Uint8Array(wavBuffer);

    // å†™å…¥ RIFF header
    wavView.setUint32(0, 0x52494646, false); // "RIFF"
    wavView.setUint32(4, 36 + totalDataSize, true); // file size - 8
    wavView.setUint32(8, 0x57415645, false); // "WAVE"

    // å†™å…¥ fmt chunk
    wavView.setUint32(12, 0x666d7420, false); // "fmt "
    wavView.setUint32(16, 16, true); // fmt chunk size
    wavView.setUint16(20, audioFormat, true); // audio format (preserve original format)
    wavView.setUint16(22, channels, true);
    wavView.setUint32(24, sampleRate, true);
    wavView.setUint32(28, sampleRate * channels * bitsPerSample / 8, true); // byte rate
    wavView.setUint16(32, channels * bitsPerSample / 8, true); // block align
    wavView.setUint16(34, bitsPerSample, true);

    // å†™å…¥ data chunk
    wavView.setUint32(36, 0x64617461, false); // "data"
    wavView.setUint32(40, totalDataSize, true);

    // åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ•°æ®
    let offset = 44;
    for (const audioData of audioDataArrays) {
        wavBytes.set(audioData, offset);
        offset += audioData.length;
    }

    console.warn(`[WAV Merge] Merge complete: ${44 + totalDataSize} bytes total`);
    return new Blob([wavBuffer], { type: 'audio/wav' });
}

/**
 * ç²¾ç®€ç‰ˆ TextToSpeech ç±» - ä»…æ”¯æŒè‹±æ–‡
 * ç”¨äº index.html (ITT é¡µé¢)ï¼Œä¸åŒ…å«ä¸­æ–‡ä¾èµ– (pinyin/pinyin2ipa)
 */
export class TextToSpeech {
    constructor() {
        this.kokoro = null;
        this.modelId = config.models.tts.useLocal
            ? config.models.tts.localPath
            : config.models.tts.id;
        this.voice = config.models.tts.defaultVoice;
        this.initialized = false;
        this.initPromise = null;
    }

    async init(progressCallback) {
        if (this.initialized) {
            return this.kokoro;
        }

        if (this.initPromise) {
            return this.initPromise;
        }

        this.initPromise = this._doInit(progressCallback);

        try {
            const result = await this.initPromise;
            this.initialized = true;
            return result;
        } catch (error) {
            this.initPromise = null;
            throw error;
        }
    }

    async _doInit(progressCallback) {
        const loadAttempts = [
            { dtype: 'fp32', device: 'webgpu', vramNeeded: '~600MB VRAM' },
            { dtype: 'fp32', device: 'wasm', vramNeeded: 'CPU mode (no GPU)' }
        ];

        const hasWebGPU = 'gpu' in navigator;
        if (!hasWebGPU) {
            console.warn('[TTS] No WebGPU support, using WASM directly');
            if (progressCallback) {
                progressCallback({
                    progress: 'Preparing',
                    stage: 'fallback',
                    dtype: 'fp32',
                    device: 'wasm',
                    attemptInfo: 'No WebGPU, using CPU mode',
                    currentAttempt: 1,
                    totalAttempts: 1
                });
            }
            return this.loadWithConfig('fp32', 'wasm', progressCallback, 1, 1);
        }

        for (let i = 0; i < loadAttempts.length; i++) {
            const { dtype, device, vramNeeded } = loadAttempts[i];

            try {
                console.log(`[TTS] ğŸ”„ Attempt ${i + 1}/${loadAttempts.length}: ${dtype.toUpperCase()}/${device.toUpperCase()} (${vramNeeded})`);

                if (progressCallback) {
                    progressCallback({
                        progress: 'Starting',
                        stage: 'attempt',
                        dtype: dtype,
                        device: device,
                        attemptInfo: `Trying ${dtype.toUpperCase()}/${device.toUpperCase()} (${vramNeeded})`,
                        currentAttempt: i + 1,
                        totalAttempts: loadAttempts.length
                    });
                }

                await this.loadWithConfig(dtype, device, progressCallback, i + 1, loadAttempts.length);

                console.log(`âœ… [TTS] Successfully loaded: ${dtype.toUpperCase()}/${device.toUpperCase()}`);
                return this.kokoro;

            } catch (error) {
                const errorMsg = error.message || '';
                console.error(`âŒ [TTS] Failed ${dtype}/${device}:`, errorMsg);

                const isOOM = errorMsg.includes('allocation') ||
                             errorMsg.includes('out of memory') ||
                             errorMsg.includes('OOM') ||
                             errorMsg.includes('CreateBuffer') ||
                             errorMsg.includes('memory');

                if (isOOM && i < loadAttempts.length - 1) {
                    const nextAttempt = loadAttempts[i + 1];
                    console.warn(`âš ï¸ [TTS] GPU memory insufficient for ${dtype}/${device}, switching to CPU mode...`);

                    if (progressCallback) {
                        progressCallback({
                            progress: 'Downgrading',
                            stage: 'fallback',
                            dtype: dtype,
                            device: device,
                            attemptInfo: `VRAM insufficient, switching to CPU mode (${nextAttempt.dtype.toUpperCase()}/${nextAttempt.device.toUpperCase()})...`,
                            currentAttempt: i + 1,
                            totalAttempts: loadAttempts.length,
                            isDowngrade: true
                        });
                    }

                    continue;
                }

                if (i === loadAttempts.length - 1) {
                    throw new Error(`âŒ Failed to load TTS model with all configurations. Last error: ${errorMsg}`);
                }

                throw error;
            }
        }
    }

    async loadWithConfig(dtype, device, progressCallback, currentAttempt, totalAttempts) {
        this.currentConfig = { dtype, device };

        const modelLoadOptions = {
            dtype: dtype,
            device: device,
            progress_callback: (progress) => {
                if (progressCallback) {
                    const percentage = progress.progress ?
                        Math.round(progress.progress) + '%' :
                        progress.status || '';

                    const fileName = progress.file || 'unknown';
                    const loaded = progress.loaded ? (progress.loaded / 1024 / 1024).toFixed(1) : 0;
                    const total = progress.total ? (progress.total / 1024 / 1024).toFixed(1) : 0;
                    const sizeInfo = total > 0 ? ` (${loaded}/${total}MB)` : '';

                    progressCallback({
                        progress: percentage,
                        stage: 'tts',
                        dtype: dtype,
                        device: device,
                        currentAttempt: currentAttempt,
                        totalAttempts: totalAttempts,
                        fileName: fileName,
                        sizeInfo: sizeInfo
                    });
                }
            }
        };

        // é•œåƒå·²åœ¨å…¨å±€ env é…ç½®ï¼Œè¿™é‡Œä¸éœ€è¦å•ç‹¬è®¾ç½®

        this.kokoro = await KokoroTTS.from_pretrained(this.modelId, modelLoadOptions);

        // å…³é”®ï¼šç§»é™¤ kokoro å†…éƒ¨çš„è¯­éŸ³éªŒè¯ï¼Œæ”¯æŒæ‰€æœ‰ 54 ä¸ªè¯­éŸ³
        if (this.kokoro && this.kokoro._validate_voice) {
            console.log('[TTS] Patching voice validation to support all 54 voices');
            const originalValidate = this.kokoro._validate_voice.bind(this.kokoro);
            this.kokoro._validate_voice = function(voice) {
                // æ‰©å±•çš„è¯­éŸ³åˆ—è¡¨ - åŒ…å«æ‰€æœ‰ 54 ä¸ªè¯­éŸ³
                const allVoices = VOICES.map(v => v.id);

                if (!allVoices.includes(voice)) {
                    throw new Error(`Voice "${voice}" not found. Should be one of: ${allVoices.join(', ')}`);
                }

                // ä¸è°ƒç”¨åŸå§‹éªŒè¯ï¼Œç›´æ¥è¿”å›
                return voice;
            };
        }
    }

    isReady() {
        return this.initialized && this.kokoro !== null;
    }

    /**
     * æ ¹æ®è¯­éŸ³åç§°è‡ªåŠ¨æ¨æ–­è¯­è¨€ä»£ç 
     */
    getLanguageFromVoice(voice) {
        const prefix = voice.substring(0, 2);
        const langMap = {
            'af': 'a',  // American English Female
            'am': 'a',  // American English Male
            'bf': 'b',  // British English Female
            'bm': 'b',  // British English Male
            'jf': 'j',  // Japanese Female
            'jm': 'j',  // Japanese Male
            'zf': 'z',  // Chinese Female (Mandarin)
            'zm': 'z',  // Chinese Male (Mandarin)
            'ef': 'e',  // Spanish Female
            'em': 'e',  // Spanish Male
            'ff': 'f',  // French Female
            'hf': 'h',  // Hindi Female
            'hm': 'h',  // Hindi Male
            'if': 'i',  // Italian Female
            'im': 'i',  // Italian Male
            'pf': 'p',  // Portuguese Female
            'pm': 'p'   // Portuguese Male
        };
        return langMap[prefix] || 'a'; // é»˜è®¤è‹±è¯­
    }

    async speak(text, voice = null, options = {}) {
        if (this.initPromise && !this.initialized) {
            console.log('[TTS] Waiting for initialization to complete...');
            await this.initPromise;
        }

        if (!this.kokoro) {
            throw new Error('TTS model not initialized. Call init() first.');
        }

        try {
            const voiceToUse = voice || this.voice;
            const onSegmentStart = options.onSegmentStart || null;
            const speed = options.speed || 1.0;  // é»˜è®¤é€Ÿåº¦ä¸º 1.0x

            // è‡ªåŠ¨æ ¹æ®è¯­éŸ³æ¨æ–­è¯­è¨€ï¼Œæˆ–ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è¯­è¨€
            const detectedLang = this.getLanguageFromVoice(voiceToUse);
            const lang = options.lang || detectedLang;

            console.warn(`[ğŸ” TTS Language Debug] Voice: ${voiceToUse}`);
            console.warn(`[ğŸ” TTS Language Debug] Auto-detected language: ${detectedLang}`);
            console.warn(`[ğŸ” TTS Language Debug] User-specified language (options.lang): ${options.lang}`);
            console.warn(`[ğŸ” TTS Language Debug] Final language to use: ${lang}`);

            // Lite ç‰ˆæœ¬ï¼šä»…æ”¯æŒè‹±æ–‡ (a/b)ï¼Œå…¶ä»–è¯­è¨€æŠ›å‡ºé”™è¯¯
            if (lang !== 'a' && lang !== 'b') {
                throw new Error(`This lite version only supports English. For other languages, please use the full TTS page.`);
            }

            const sentences = [];
            const allSentences = text.split(/(?<=[.!?])\s+/).filter(s => s.trim().length > 0);

            const sentencesPerSegment = 1;
            for (let i = 0; i < allSentences.length; i += sentencesPerSegment) {
                const segment = allSentences.slice(i, i + sentencesPerSegment).join(' ');
                sentences.push(segment);
            }

            console.log(`[TTS] Splitting text into ${sentences.length} segments`);

            const audioQueue = [];
            const allAudioBlobs = [];  // æ”¶é›†æ‰€æœ‰éŸ³é¢‘ blob ç”¨äºä¸‹è½½
            let currentAudio = null;
            let isStopped = false;
            let isPlaying = false;
            let generationComplete = false;

            const playNext = async () => {
                if (isStopped) return;
                if (isPlaying) return;

                while (!isStopped && audioQueue.length === 0 && !generationComplete) {
                    await new Promise(resolve => setTimeout(resolve, 50));
                }

                if (isStopped || audioQueue.length === 0) {
                    isPlaying = false;
                    return;
                }

                isPlaying = true;
                const { audio, url, index, text: segmentText } = audioQueue.shift();
                currentAudio = audio;

                // è®¾ç½®æ’­æ”¾é€Ÿåº¦
                audio.playbackRate = speed;

                if (onSegmentStart) {
                    onSegmentStart(index, segmentText, sentences);
                }

                try {
                    await audio.play();

                    audio.addEventListener('ended', () => {
                        setTimeout(() => {
                            URL.revokeObjectURL(url);
                        }, 1000);
                        isPlaying = false;
                        playNext();
                    }, { once: true });
                } catch (error) {
                    console.error('[TTS] Error playing audio segment:', error);
                    setTimeout(() => {
                        URL.revokeObjectURL(url);
                    }, 1000);
                    isPlaying = false;
                    playNext();
                }
            };

            const generateAndPlay = async () => {
                for (let i = 0; i < sentences.length; i++) {
                    if (isStopped) break;

                    const sentence = sentences[i].trim();
                    if (!sentence) continue;

                    const segmentStartTime = performance.now();
                    console.warn(`[â±ï¸ TTS Segment ${i + 1}/${sentences.length}] Starting generation (${sentence.length} chars)`);

                    let audioUrl = null;
                    try {
                        const generateStartTime = performance.now();

                        console.warn(`[ğŸ” TTS Generation] Calling generation with text: "${sentence.substring(0, 50)}..."`);
                        console.warn(`[ğŸ” TTS Generation] Voice: ${voiceToUse}, Language code: ${lang}`);

                        // è‹±è¯­ï¼šä½¿ç”¨ generate æ–¹æ³•ï¼ˆåŒ…å«éŸ³ç´ åŒ–ï¼‰
                        console.warn(`[ğŸ” TTS Generation] Using phonemization for English`);
                        const audioOutput = await this.kokoro.generate(sentence, {
                            voice: voiceToUse
                        });
                        console.warn(`[ğŸ” TTS Generation] English phonemization complete`);

                        const generateEndTime = performance.now();
                        const generateDuration = (generateEndTime - generateStartTime).toFixed(0);

                        const blobStartTime = performance.now();
                        const wavBlob = audioOutput.toBlob();
                        const blobEndTime = performance.now();
                        const blobDuration = (blobEndTime - blobStartTime).toFixed(0);

                        audioUrl = URL.createObjectURL(wavBlob);
                        const audio = new Audio(audioUrl);

                        const segmentEndTime = performance.now();
                        const segmentTotalDuration = (segmentEndTime - segmentStartTime).toFixed(0);

                        console.warn(`[â±ï¸ TTS Segment ${i + 1}] Generated in ${segmentTotalDuration}ms (generate: ${generateDuration}ms, blob: ${blobDuration}ms)`);

                        allAudioBlobs.push(wavBlob);  // ä¿å­˜ blob ç”¨äºä¸‹è½½
                        audioQueue.push({ audio, url: audioUrl, index: i, text: sentence });

                        if (i === 0) {
                            console.warn('[â±ï¸ TTS] First segment ready, starting playback');
                            playNext();
                        }
                    } catch (error) {
                        const segmentEndTime = performance.now();
                        const segmentTotalDuration = (segmentEndTime - segmentStartTime).toFixed(0);
                        console.error(`[â±ï¸ TTS Segment ${i + 1}] Error after ${segmentTotalDuration}ms:`, error);
                        if (audioUrl) {
                            setTimeout(() => {
                                URL.revokeObjectURL(audioUrl);
                            }, 1000);
                        }
                    }
                }
                generationComplete = true;
                console.warn('[â±ï¸ TTS] All segments generated');
            };

            generateAndPlay();

            const controller = {
                get paused() {
                    return currentAudio ? currentAudio.paused : true;
                },
                pause() {
                    if (currentAudio) currentAudio.pause();
                },
                play() {
                    if (currentAudio) currentAudio.play();
                },
                stop() {
                    isStopped = true;
                    if (currentAudio) {
                        currentAudio.pause();
                        currentAudio.currentTime = 0;
                    }
                    audioQueue.forEach(({ url }) => {
                        setTimeout(() => {
                            URL.revokeObjectURL(url);
                        }, 1000);
                    });
                    audioQueue.length = 0;
                },
                addEventListener(event, handler) {
                    if (event === 'ended') {
                        let hasTriggered = false;
                        const checkEnded = () => {
                            if (audioQueue.length === 0 && currentAudio && currentAudio.ended) {
                                if (!hasTriggered) {
                                    hasTriggered = true;
                                    clearInterval(interval);
                                    handler();
                                }
                            }
                        };
                        const interval = setInterval(checkEnded, 100);
                        // 5åˆ†é’Ÿåå¼ºåˆ¶æ¸…ç†ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                        setTimeout(() => {
                            if (!hasTriggered) {
                                clearInterval(interval);
                            }
                        }, 300000);
                    }
                },
                async getAudioBlob() {
                    // ç­‰å¾…æ‰€æœ‰éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼ˆä¸éœ€è¦ç­‰æ’­æ”¾å®Œæˆï¼‰
                    while (!generationComplete && !isStopped) {
                        await new Promise(resolve => setTimeout(resolve, 100));
                    }

                    if (allAudioBlobs.length === 0) return null;
                    if (allAudioBlobs.length === 1) return allAudioBlobs[0];

                    // åˆå¹¶å¤šä¸ªéŸ³é¢‘ç‰‡æ®µ
                    console.warn('[TTS Download] Merging', allAudioBlobs.length, 'audio segments...');
                    return await mergeWavBlobs(allAudioBlobs);
                },
                getAllAudioBlobs() {
                    // è¿”å›æ‰€æœ‰éŸ³é¢‘ blobs
                    return allAudioBlobs;
                }
            };

            return controller;
        } catch (error) {
            console.error('Error generating speech:', error);
            throw error;
        }
    }

    setVoice(voice) {
        this.voice = voice;
    }

    getAvailableVoices() {
        // è¿”å›æ‰€æœ‰ 54 ä¸ªè¯­éŸ³
        return VOICES.map(v => v.id);
    }
}
