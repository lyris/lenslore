import { KokoroTTS } from 'kokoro-js';
import { config } from './config.js';
import { env } from '@huggingface/transformers';
import { pinyin } from 'pinyin';
// ä½¿ç”¨æµè§ˆå™¨æ„å»ºç‰ˆæœ¬çš„ pinyin2ipa (UMD æ ¼å¼)
// æ³¨æ„: Vite ä¼šé€šè¿‡ alias é…ç½®å°†å…¶æŒ‡å‘ dist/pinyin2ipa.js
import pinyin2ipaModule from 'pinyin2ipa';
const pinyin2ipa = pinyin2ipaModule.default || pinyin2ipaModule;
import { VOICES } from './voices.js';

// å¯ç”¨æµè§ˆå™¨ç¼“å­˜ä¼˜å…ˆç­–ç•¥ï¼ˆä¼˜å…ˆä½¿ç”¨ Cache API å’Œ Service Workerï¼‰
env.useBrowserCache = true;
env.useFSCache = true;

// é…ç½®é•œåƒæœåŠ¡å™¨ï¼ˆå¿…é¡»åœ¨å…¨å±€ env è®¾ç½®ï¼‰
if (config.mirror.enabled && config.mirror.url) {
    // transformers.js v3 éœ€è¦åŒæ—¶è®¾ç½®è¿™äº›å±æ€§
    env.remoteHost = config.mirror.url;
    env.remotePathTemplate = '{model}/resolve/{revision}/';

    // å¯¹äº HuggingFace é•œåƒï¼Œéœ€è¦ç¡®ä¿ URL æ ¼å¼æ­£ç¡®
    // å¦‚æœé•œåƒ URL åŒ…å«å®Œæ•´è·¯å¾„æ¨¡æ¿ï¼Œç›´æ¥ä½¿ç”¨
    if (config.mirror.url.includes('{model}')) {
        env.remotePathTemplate = config.mirror.url;
        env.remoteHost = '';
    }

    console.warn(`[TextToSpeech] âœ… Global mirror configured:`);
    console.warn(`[TextToSpeech]   remoteHost: ${env.remoteHost}`);
    console.warn(`[TextToSpeech]   remotePathTemplate: ${env.remotePathTemplate}`);
    console.warn(`[TextToSpeech]   Expected URL format: ${env.remoteHost}/${env.remotePathTemplate.replace('{model}', 'MODEL_ID').replace('{revision}', 'REVISION')}`);

    // éªŒè¯ env å¯¹è±¡æ˜¯å¦è¢«æ­£ç¡®è®¾ç½®
    console.warn(`[TextToSpeech] env object check:`, {
        useBrowserCache: env.useBrowserCache,
        useFSCache: env.useFSCache,
        remoteHost: env.remoteHost,
        remotePathTemplate: env.remotePathTemplate,
        backends: env.backends
    });
}



/**
 * åˆå¹¶å¤šä¸ª WAV Blob æ–‡ä»¶
 * @param {Blob[]} blobs - è¦åˆå¹¶çš„ WAV Blob æ•°ç»„
 * @returns {Promise<Blob>} åˆå¹¶åçš„ WAV Blob
 */
async function mergeWavBlobs(blobs) {
    if (blobs.length === 0) return null;
    if (blobs.length === 1) return blobs[0];

    console.warn(`[WAV Merge] Merging ${blobs.length} WAV files...`);

    // è¯»å–æ‰€æœ‰ WAV æ–‡ä»¶çš„ ArrayBuffer
    const buffers = await Promise.all(blobs.map(blob => blob.arrayBuffer()));

    // è§£æç¬¬ä¸€ä¸ª WAV æ–‡ä»¶çš„å¤´éƒ¨ä¿¡æ¯
    const firstBuffer = buffers[0];
    const view = new DataView(firstBuffer);

    // æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ WAV æ–‡ä»¶
    const riff = String.fromCharCode(view.getUint8(0), view.getUint8(1), view.getUint8(2), view.getUint8(3));
    if (riff !== 'RIFF') {
        console.error('[WAV Merge] Invalid WAV file: RIFF header not found');
        return blobs[0];
    }

    // è¯»å–éŸ³é¢‘å‚æ•°
    const channels = view.getUint16(22, true);
    const sampleRate = view.getUint32(24, true);
    const bitsPerSample = view.getUint16(34, true);

    console.warn(`[WAV Merge] Audio format: ${channels} channels, ${sampleRate}Hz, ${bitsPerSample}bit`);

    // æ£€æŸ¥éŸ³é¢‘æ ¼å¼
    const audioFormat = view.getUint16(20, true);
    const isFloat = audioFormat === 3; // 3 = IEEE float
    console.warn(`[WAV Merge] Audio format code: ${audioFormat} (${isFloat ? 'IEEE Float' : 'PCM'})`);

    // æå–æ‰€æœ‰æ–‡ä»¶çš„éŸ³é¢‘æ•°æ®ï¼ˆè·³è¿‡ WAV å¤´éƒ¨ï¼Œæ ‡å‡† WAV å¤´éƒ¨æ˜¯ 44 å­—èŠ‚ï¼‰
    const audioDataArrays = buffers.map((buffer, index) => {
        // æ ‡å‡† WAV æ–‡ä»¶å¤´éƒ¨æ˜¯ 44 å­—èŠ‚
        const headerSize = 44;
        const dataSize = buffer.byteLength - headerSize;
        const audioData = new Uint8Array(buffer, headerSize, dataSize);
        console.warn(`[WAV Merge] File ${index + 1}: ${dataSize} bytes of audio data`);
        return audioData;
    });

    // è®¡ç®—åˆå¹¶åçš„æ€»å¤§å°
    const totalDataSize = audioDataArrays.reduce((sum, arr) => sum + arr.length, 0);
    console.warn(`[WAV Merge] Total audio data: ${totalDataSize} bytes`);

    // åˆ›å»ºæ–°çš„ WAV æ–‡ä»¶
    const wavBuffer = new ArrayBuffer(44 + totalDataSize);
    const wavView = new DataView(wavBuffer);
    const wavBytes = new Uint8Array(wavBuffer);

    // å†™å…¥ RIFF header
    wavView.setUint32(0, 0x52494646, false); // "RIFF"
    wavView.setUint32(4, 36 + totalDataSize, true); // file size - 8
    wavView.setUint32(8, 0x57415645, false); // "WAVE"

    // å†™å…¥ fmt chunk
    wavView.setUint32(12, 0x666d7420, false); // "fmt "
    wavView.setUint32(16, 16, true); // fmt chunk size
    wavView.setUint16(20, audioFormat, true); // audio format (preserve original format)
    wavView.setUint16(22, channels, true);
    wavView.setUint32(24, sampleRate, true);
    wavView.setUint32(28, sampleRate * channels * bitsPerSample / 8, true); // byte rate
    wavView.setUint16(32, channels * bitsPerSample / 8, true); // block align
    wavView.setUint16(34, bitsPerSample, true);

    // å†™å…¥ data chunk
    wavView.setUint32(36, 0x64617461, false); // "data"
    wavView.setUint32(40, totalDataSize, true);

    // åˆå¹¶æ‰€æœ‰éŸ³é¢‘æ•°æ®
    let offset = 44;
    for (const audioData of audioDataArrays) {
        wavBytes.set(audioData, offset);
        offset += audioData.length;
    }

    console.warn(`[WAV Merge] Merge complete: ${44 + totalDataSize} bytes total`);
    return new Blob([wavBuffer], { type: 'audio/wav' });
}

/**
 * æ‰©å±•çš„ TextToSpeech ç±»ï¼Œæ”¯æŒæ‰€æœ‰ 54 ä¸ª Kokoro è¯­éŸ³
 * é€šè¿‡ç§»é™¤è¯­éŸ³éªŒè¯æ¥æ”¯æŒåŸå§‹æ¨¡å‹çš„æ‰€æœ‰è¯­éŸ³
 */
export class TextToSpeech {
    constructor() {
        this.kokoro = null;
        this.modelId = config.models.tts.useLocal
            ? config.models.tts.localPath
            : config.models.tts.id;
        this.voice = config.models.tts.defaultVoice;
        this.initialized = false;
        this.initPromise = null;
    }

    async init(progressCallback) {
        if (this.initialized) {
            return this.kokoro;
        }

        if (this.initPromise) {
            return this.initPromise;
        }

        this.initPromise = this._doInit(progressCallback);

        try {
            const result = await this.initPromise;
            this.initialized = true;
            return result;
        } catch (error) {
            this.initPromise = null;
            throw error;
        }
    }

    async _doInit(progressCallback) {
        // æ ¹æ®ç”¨æˆ·é…ç½®é€‰æ‹©è®¾å¤‡ï¼ˆä» config æˆ– localStorage è¯»å–ï¼‰
        const devicePreference = config.models.tts?.devicePreference || 'wasm'; // é»˜è®¤ WASMï¼ˆæ›´ç¨³å®šï¼‰
        const hasWebGPU = 'gpu' in navigator;

        let loadAttempts = [];

        if (devicePreference === 'webgpu') {
            // ç”¨æˆ·å¼ºåˆ¶ä½¿ç”¨ WebGPU
            if (!hasWebGPU) {
                throw new Error('WebGPU not supported in this browser. Please switch to Auto or WASM mode in Settings.');
            }
            loadAttempts = [{ dtype: 'fp32', device: 'webgpu', vramNeeded: '~600MB VRAM' }];
            console.warn('[TTS] User preference: WebGPU mode');
        } else if (devicePreference === 'wasm') {
            // ç”¨æˆ·å¼ºåˆ¶ä½¿ç”¨ WASM
            loadAttempts = [{ dtype: 'fp32', device: 'wasm', vramNeeded: 'CPU mode (no GPU)' }];
            console.warn('[TTS] User preference: WASM mode (most stable)');
        } else {
            // è‡ªåŠ¨æ¨¡å¼ï¼šä¼˜å…ˆ WASMï¼ˆå› ä¸º WebGPU æœ‰å·²çŸ¥é—®é¢˜ï¼‰ï¼Œå¤±è´¥åˆ™å°è¯• WebGPU
            // æ³¨æ„ï¼šè¿™é‡Œä¼˜å…ˆ WASM è€Œä¸æ˜¯ WebGPUï¼Œå› ä¸º WebGPU æœ‰ Float32Array å¯¹é½é—®é¢˜
            if (hasWebGPU) {
                loadAttempts = [
                    { dtype: 'fp32', device: 'wasm', vramNeeded: 'CPU mode (no GPU)' },
                    { dtype: 'fp32', device: 'webgpu', vramNeeded: '~600MB VRAM' }
                ];
                console.warn('[TTS] Auto mode: trying WASM first (WebGPU has known stability issues)');
            } else {
                loadAttempts = [{ dtype: 'fp32', device: 'wasm', vramNeeded: 'CPU mode (no GPU)' }];
                console.warn('[TTS] No WebGPU support, using WASM directly');
            }
        }

        // å¦‚æœåªæœ‰ä¸€ä¸ªé€‰é¡¹æˆ–å¼ºåˆ¶æ¨¡å¼ï¼Œç›´æ¥åŠ è½½
        if (loadAttempts.length === 1 || devicePreference !== 'auto') {
            const { dtype, device } = loadAttempts[0];
            if (progressCallback) {
                progressCallback({
                    progress: 'Preparing',
                    stage: 'fallback',
                    dtype: dtype,
                    device: device,
                    attemptInfo: `Using ${device.toUpperCase()} mode`,
                    currentAttempt: 1,
                    totalAttempts: 1
                });
            }
            return this.loadWithConfig(dtype, device, progressCallback, 1, 1);
        }

        for (let i = 0; i < loadAttempts.length; i++) {
            const { dtype, device, vramNeeded } = loadAttempts[i];

            try {
                console.log(`[TTS] ğŸ”„ Attempt ${i + 1}/${loadAttempts.length}: ${dtype.toUpperCase()}/${device.toUpperCase()} (${vramNeeded})`);

                if (progressCallback) {
                    progressCallback({
                        progress: 'Starting',
                        stage: 'attempt',
                        dtype: dtype,
                        device: device,
                        attemptInfo: `Trying ${dtype.toUpperCase()}/${device.toUpperCase()} (${vramNeeded})`,
                        currentAttempt: i + 1,
                        totalAttempts: loadAttempts.length
                    });
                }

                await this.loadWithConfig(dtype, device, progressCallback, i + 1, loadAttempts.length);

                console.log(`âœ… [TTS] Successfully loaded: ${dtype.toUpperCase()}/${device.toUpperCase()}`);
                return this.kokoro;

            } catch (error) {
                const errorMsg = error.message || '';
                console.error(`âŒ [TTS] Failed ${dtype}/${device}:`, errorMsg);

                const isOOM = errorMsg.includes('allocation') ||
                             errorMsg.includes('out of memory') ||
                             errorMsg.includes('OOM') ||
                             errorMsg.includes('CreateBuffer') ||
                             errorMsg.includes('memory');

                if (isOOM && i < loadAttempts.length - 1) {
                    const nextAttempt = loadAttempts[i + 1];
                    console.warn(`âš ï¸ [TTS] GPU memory insufficient for ${dtype}/${device}, switching to CPU mode...`);

                    if (progressCallback) {
                        progressCallback({
                            progress: 'Downgrading',
                            stage: 'fallback',
                            dtype: dtype,
                            device: device,
                            attemptInfo: `VRAM insufficient, switching to CPU mode (${nextAttempt.dtype.toUpperCase()}/${nextAttempt.device.toUpperCase()})...`,
                            currentAttempt: i + 1,
                            totalAttempts: loadAttempts.length,
                            isDowngrade: true
                        });
                    }

                    continue;
                }

                if (i === loadAttempts.length - 1) {
                    throw new Error(`âŒ Failed to load TTS model with all configurations. Last error: ${errorMsg}`);
                }

                throw error;
            }
        }
    }

    async loadWithConfig(dtype, device, progressCallback, currentAttempt, totalAttempts) {
        this.currentConfig = { dtype, device };

        const modelLoadOptions = {
            dtype: dtype,
            device: device,
            progress_callback: (progress) => {
                if (progressCallback) {
                    const percentage = progress.progress ?
                        Math.round(progress.progress) + '%' :
                        progress.status || '';

                    const fileName = progress.file || 'unknown';
                    const loaded = progress.loaded ? (progress.loaded / 1024 / 1024).toFixed(1) : 0;
                    const total = progress.total ? (progress.total / 1024 / 1024).toFixed(1) : 0;
                    const sizeInfo = total > 0 ? ` (${loaded}/${total}MB)` : '';

                    progressCallback({
                        progress: percentage,
                        stage: 'tts',
                        dtype: dtype,
                        device: device,
                        currentAttempt: currentAttempt,
                        totalAttempts: totalAttempts,
                        fileName: fileName,
                        sizeInfo: sizeInfo
                    });
                }
            }
        };

        // åœ¨åŠ è½½é€‰é¡¹ä¸­ä¹Ÿæ·»åŠ é•œåƒé…ç½®ï¼ˆæŸäº›ç‰ˆæœ¬çš„ transformers.js éœ€è¦è¿™æ ·åšï¼‰
        if (config.mirror.enabled && config.mirror.url) {
            // å°è¯•é€šè¿‡ session é€‰é¡¹ä¼ é€’é•œåƒé…ç½®
            modelLoadOptions.session = {
                remoteHost: config.mirror.url,
                remotePathTemplate: '{model}/resolve/{revision}/'
            };
            console.warn(`[TextToSpeech] Adding mirror config to model load options`);
        }

        this.kokoro = await KokoroTTS.from_pretrained(this.modelId, modelLoadOptions);

        // å…³é”®ï¼šç§»é™¤ kokoro å†…éƒ¨çš„è¯­éŸ³éªŒè¯ï¼Œæ”¯æŒæ‰€æœ‰ 54 ä¸ªè¯­éŸ³
        if (this.kokoro && this.kokoro._validate_voice) {
            console.warn('[TTS] Patching voice validation to support all 54 voices');
            this.kokoro._validate_voice = function(voice) {
                // æ‰©å±•çš„è¯­éŸ³åˆ—è¡¨ - åŒ…å«æ‰€æœ‰ 54 ä¸ªè¯­éŸ³
                const allVoices = VOICES.map(v => v.id);

                if (!allVoices.includes(voice)) {
                    throw new Error(`Voice "${voice}" not found. Should be one of: ${allVoices.join(', ')}`);
                }

                // è¿”å›è¯­éŸ³IDçš„é¦–å­—ç¬¦ï¼ˆè¯­è¨€ä»£ç ï¼š'a' for American English, 'b' for British, etcï¼‰
                // è¿™æ˜¯ kokoro.js å†…éƒ¨ generate() æ–¹æ³•éœ€è¦çš„è¿”å›å€¼
                return voice.at(0);
            };
        }

        // æµ‹è¯•ï¼šå°è¯•å®é™…ç”Ÿæˆä¸€å°æ®µéŸ³é¢‘æ¥éªŒè¯
        try {
            console.warn('[TTS] Testing audio generation with short text...');
            const testVoice = config.models.tts.defaultVoice || 'af_heart';
            const testText = "Hi";  // éå¸¸çŸ­çš„æ–‡æœ¬

            // å°è¯•å®é™…ç”ŸæˆéŸ³é¢‘
            const testAudio = await this.kokoro.generate(testText, { voice: testVoice });
            console.warn('[TTS] Test generation successful, audio type:', testAudio.constructor.name);
            console.warn('[TTS] Audio has toBlob method:', typeof testAudio.toBlob === 'function');
        } catch (testError) {
            console.error('[TTS] Test generation failed:', testError);
            console.error('[TTS] Error message:', testError.message);
            console.error('[TTS] This indicates a problem with kokoro.js or the voice files');
        }
    }

    isReady() {
        return this.initialized && this.kokoro !== null;
    }

    /**
     * ä¸­æ–‡æ–‡æœ¬è½¬ IPA phonemes
     * @param {string} text ä¸­æ–‡æ–‡æœ¬
     * @returns {string} IPA phonemes
     */
    chineseToIPA(text) {
        try {
            // ç¬¬ä¸€æ­¥ï¼šä¸­æ–‡å­—ç¬¦ â†’ pinyin (with tone numbers like zhong1)
            const pinyinResult = pinyin(text, {
                style: pinyin.STYLE_TONE2,  // æ•°å­—å£°è°ƒçš„æ‹¼éŸ³ (zhong1 guo2) - pinyin2ipaæ›´å¥½æ”¯æŒ
                heteronym: false,            // ä¸æ˜¾ç¤ºå¤šéŸ³å­—çš„æ‰€æœ‰è¯»éŸ³
                segment: true                // å¯ç”¨åˆ†è¯
            });

            // pinyin() è¿”å›äºŒç»´æ•°ç»„: [['zhong1'], ['guo2']]
            // å°†å…¶æ‰å¹³åŒ–å¹¶ç”¨ç©ºæ ¼è¿æ¥: "zhong1 guo2"
            const pinyinText = pinyinResult.map(item => item[0]).join(' ');

            console.warn(`[ğŸ” Chinese G2P] Original text: "${text}"`);
            console.warn(`[ğŸ” Chinese G2P] Pinyin (TONE2): "${pinyinText}"`);

            // ç¬¬äºŒæ­¥ï¼špinyin â†’ IPA (é€ä¸ªè½¬æ¢ä»¥å¤„ç†å¤±è´¥çš„æƒ…å†µ)
            const pinyinArray = pinyinText.split(' ');
            const ipaArray = [];

            for (let py of pinyinArray) {
                const ipa = pinyin2ipa(py);
                if (ipa && ipa.trim()) {
                    ipaArray.push(ipa.trim());
                } else {
                    // å¦‚æœ pinyin2ipa æ— æ³•è½¬æ¢ï¼Œä½¿ç”¨åŸå§‹æ‹¼éŸ³
                    console.warn(`[ğŸ” Chinese G2P] Warning: Failed to convert "${py}", using original`);
                    ipaArray.push(py);
                }
            }

            let ipaText = ipaArray.join(' ');

            // åœ¨ IPA æœ«å°¾æ·»åŠ å¥å­ç»“æŸæ ‡è®°ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«å¥å­è¾¹ç•Œ
            ipaText = ipaText.trim() + ' .';

            console.warn(`[ğŸ” Chinese G2P] IPA: "${ipaText}"`);

            return ipaText;
        } catch (error) {
            console.error('[Chinese G2P] Error:', error);
            // å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸæ–‡æœ¬
            return text;
        }
    }

    /**
     * æ ¹æ®è¯­éŸ³åç§°è‡ªåŠ¨æ¨æ–­è¯­è¨€ä»£ç 
     */
    getLanguageFromVoice(voice) {
        const prefix = voice.substring(0, 2);
        const langMap = {
            'af': 'a',  // American English Female
            'am': 'a',  // American English Male
            'bf': 'b',  // British English Female
            'bm': 'b',  // British English Male
            'jf': 'j',  // Japanese Female
            'jm': 'j',  // Japanese Male
            'zf': 'z',  // Chinese Female (Mandarin)
            'zm': 'z',  // Chinese Male (Mandarin)
            'ef': 'e',  // Spanish Female
            'em': 'e',  // Spanish Male
            'ff': 'f',  // French Female
            'hf': 'h',  // Hindi Female
            'hm': 'h',  // Hindi Male
            'if': 'i',  // Italian Female
            'im': 'i',  // Italian Male
            'pf': 'p',  // Portuguese Female
            'pm': 'p'   // Portuguese Male
        };
        return langMap[prefix] || 'a'; // é»˜è®¤è‹±è¯­
    }

    async speak(text, voice = null, options = {}) {
        if (this.initPromise && !this.initialized) {
            console.log('[TTS] Waiting for initialization to complete...');
            await this.initPromise;
        }

        if (!this.kokoro) {
            throw new Error('TTS model not initialized. Call init() first.');
        }

        try {
            const voiceToUse = voice || this.voice;
            const onSegmentStart = options.onSegmentStart || null;
            const speed = options.speed || 1.0;  // é»˜è®¤é€Ÿåº¦ä¸º 1.0x

            // è‡ªåŠ¨æ ¹æ®è¯­éŸ³æ¨æ–­è¯­è¨€ï¼Œæˆ–ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è¯­è¨€
            const detectedLang = this.getLanguageFromVoice(voiceToUse);
            const lang = options.lang || detectedLang;

            console.warn(`[ğŸ” TTS Language Debug] Voice: ${voiceToUse}`);
            console.warn(`[ğŸ” TTS Language Debug] Auto-detected language: ${detectedLang}`);
            console.warn(`[ğŸ” TTS Language Debug] User-specified language (options.lang): ${options.lang}`);
            console.warn(`[ğŸ” TTS Language Debug] Final language to use: ${lang}`);

            const sentences = [];
            const allSentences = text.split(/(?<=[.!?])\s+/).filter(s => s.trim().length > 0);

            const sentencesPerSegment = 1;
            for (let i = 0; i < allSentences.length; i += sentencesPerSegment) {
                const segment = allSentences.slice(i, i + sentencesPerSegment).join(' ');
                sentences.push(segment);
            }

            console.log(`[TTS] Splitting text into ${sentences.length} segments`);

            const audioQueue = [];
            const allAudioBlobs = [];  // æ”¶é›†æ‰€æœ‰éŸ³é¢‘ blob ç”¨äºä¸‹è½½
            let currentAudio = null;
            let isStopped = false;
            let isPlaying = false;
            let generationComplete = false;

            const playNext = async () => {
                if (isStopped) return;
                if (isPlaying) return;

                while (!isStopped && audioQueue.length === 0 && !generationComplete) {
                    await new Promise(resolve => setTimeout(resolve, 50));
                }

                if (isStopped || audioQueue.length === 0) {
                    isPlaying = false;
                    return;
                }

                isPlaying = true;
                const { audio, url, index, text: segmentText } = audioQueue.shift();
                currentAudio = audio;

                // è®¾ç½®æ’­æ”¾é€Ÿåº¦
                audio.playbackRate = speed;

                if (onSegmentStart) {
                    onSegmentStart(index, segmentText, sentences);
                }

                try {
                    await audio.play();

                    audio.addEventListener('ended', () => {
                        setTimeout(() => {
                            URL.revokeObjectURL(url);
                        }, 1000);
                        isPlaying = false;
                        playNext();
                    }, { once: true });
                } catch (error) {
                    console.error('[TTS] Error playing audio segment:', error);
                    setTimeout(() => {
                        URL.revokeObjectURL(url);
                    }, 1000);
                    isPlaying = false;
                    playNext();
                }
            };

            const generateAndPlay = async () => {
                for (let i = 0; i < sentences.length; i++) {
                    if (isStopped) break;

                    const sentence = sentences[i].trim();
                    if (!sentence) continue;

                    const segmentStartTime = performance.now();
                    console.warn(`[â±ï¸ TTS Segment ${i + 1}/${sentences.length}] Starting generation (${sentence.length} chars)`);

                    let audioUrl = null;
                    try {
                        const generateStartTime = performance.now();

                        console.warn(`[ğŸ” TTS Generation] Calling generation with text: "${sentence.substring(0, 50)}..."`);
                        console.warn(`[ğŸ” TTS Generation] Voice: ${voiceToUse}, Language code: ${lang}`);
                        console.warn(`[ğŸ” TTS Generation] Language code type: ${typeof lang}, value: "${lang}"`);
                        console.warn(`[ğŸ” TTS Generation] Comparison check: lang === 'a'? ${lang === 'a'}, lang === 'b'? ${lang === 'b'}`);

                        let audioOutput;

                        // æ‰€æœ‰è¯­è¨€ç»Ÿä¸€ä½¿ç”¨ generate æ–¹æ³•ï¼ˆkokoro ä¼šè‡ªåŠ¨å¤„ç†éŸ³ç´ åŒ–ï¼‰
                        console.warn(`[ğŸ” TTS Generation] Calling generate method...`);
                        try {
                            audioOutput = await this.kokoro.generate(sentence, {
                                voice: voiceToUse
                            });
                            console.warn(`[ğŸ” TTS Generation] Generate complete, audio type:`, audioOutput.constructor.name);
                        } catch (generateError) {
                            console.error(`[ğŸ” TTS Generation] Generate failed:`, generateError);
                            console.error(`[ğŸ” TTS Generation] Error stack:`, generateError.stack);
                            throw generateError;
                        }

                        // ä¿ç•™æ—§çš„åˆ†æ”¯é€»è¾‘ä»¥ä¾¿è°ƒè¯•
                        if (false && (lang === 'a' || lang === 'b')) {
                            // è‹±è¯­ï¼šä½¿ç”¨ generate æ–¹æ³•ï¼ˆåŒ…å«éŸ³ç´ åŒ–ï¼‰
                            console.warn(`[ğŸ” TTS Generation] Branch: Using phonemization for English`);
                            audioOutput = await this.kokoro.generate(sentence, {
                                voice: voiceToUse
                            });
                            console.warn(`[ğŸ” TTS Generation] English phonemization complete`);
                        } else if (false) {
                            // éè‹±è¯­ï¼šéœ€è¦ç‰¹æ®Šå¤„ç†
                            console.warn(`[ğŸ” TTS Generation] Branch: Non-English language (${lang})`);

                            let textToTokenize = sentence;

                            // ä¸­æ–‡éœ€è¦ç‰¹æ®Šçš„ G2P è½¬æ¢
                            if (lang === 'z') {
                                console.warn(`[ğŸ” TTS Generation] Chinese detected, performing G2P conversion...`);
                                textToTokenize = this.chineseToIPA(sentence);
                                console.warn(`[ğŸ” TTS Generation] After G2P: "${textToTokenize}"`);
                            } else {
                                console.warn(`[ğŸ” TTS Generation] Language ${lang} - using direct text tokenization`);
                            }

                            // Tokenize IPA/phonemes
                            const tokenResult = this.kokoro.tokenizer(textToTokenize, {
                                truncation: true
                            });
                            console.warn(`[ğŸ” TTS Generation] input_ids dims:`, tokenResult.input_ids.dims);
                            console.warn(`[ğŸ” TTS Generation] input_ids data (first 20):`, Array.from(tokenResult.input_ids.data).slice(0, 20));

                            audioOutput = await this.kokoro.generate_from_ids(tokenResult.input_ids, {
                                voice: voiceToUse
                            });
                            console.warn(`[ğŸ” TTS Generation] Non-English generation complete`);
                        }

                        console.warn(`[ğŸ” TTS Generation] Audio generated successfully`);

                        const generateEndTime = performance.now();
                        const generateDuration = (generateEndTime - generateStartTime).toFixed(0);

                        const blobStartTime = performance.now();
                        const wavBlob = audioOutput.toBlob();
                        const blobEndTime = performance.now();
                        const blobDuration = (blobEndTime - blobStartTime).toFixed(0);

                        audioUrl = URL.createObjectURL(wavBlob);
                        const audio = new Audio(audioUrl);

                        const segmentEndTime = performance.now();
                        const segmentTotalDuration = (segmentEndTime - segmentStartTime).toFixed(0);

                        console.warn(`[â±ï¸ TTS Segment ${i + 1}] Generated in ${segmentTotalDuration}ms (generate: ${generateDuration}ms, blob: ${blobDuration}ms)`);

                        allAudioBlobs.push(wavBlob);  // ä¿å­˜ blob ç”¨äºä¸‹è½½
                        audioQueue.push({ audio, url: audioUrl, index: i, text: sentence });

                        if (i === 0) {
                            console.warn('[â±ï¸ TTS] First segment ready, starting playback');
                            playNext();
                        }
                    } catch (error) {
                        const segmentEndTime = performance.now();
                        const segmentTotalDuration = (segmentEndTime - segmentStartTime).toFixed(0);
                        console.error(`[â±ï¸ TTS Segment ${i + 1}] Error after ${segmentTotalDuration}ms:`, error);
                        if (audioUrl) {
                            setTimeout(() => {
                                URL.revokeObjectURL(audioUrl);
                            }, 1000);
                        }
                    }
                }
                generationComplete = true;
                console.warn('[â±ï¸ TTS] All segments generated');
            };

            generateAndPlay();

            const controller = {
                get paused() {
                    return currentAudio ? currentAudio.paused : true;
                },
                pause() {
                    if (currentAudio) currentAudio.pause();
                },
                play() {
                    if (currentAudio) currentAudio.play();
                },
                stop() {
                    isStopped = true;
                    if (currentAudio) {
                        currentAudio.pause();
                        currentAudio.currentTime = 0;
                    }
                    audioQueue.forEach(({ url }) => {
                        setTimeout(() => {
                            URL.revokeObjectURL(url);
                        }, 1000);
                    });
                    audioQueue.length = 0;
                },
                addEventListener(event, handler) {
                    if (event === 'ended') {
                        let hasTriggered = false;
                        const checkEnded = () => {
                            if (audioQueue.length === 0 && currentAudio && currentAudio.ended) {
                                if (!hasTriggered) {
                                    hasTriggered = true;
                                    clearInterval(interval);
                                    handler();
                                }
                            }
                        };
                        const interval = setInterval(checkEnded, 100);
                        // 5åˆ†é’Ÿåå¼ºåˆ¶æ¸…ç†ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                        setTimeout(() => {
                            if (!hasTriggered) {
                                clearInterval(interval);
                            }
                        }, 300000);
                    }
                },
                async getAudioBlob() {
                    // ç­‰å¾…æ‰€æœ‰éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼ˆä¸éœ€è¦ç­‰æ’­æ”¾å®Œæˆï¼‰
                    while (!generationComplete && !isStopped) {
                        await new Promise(resolve => setTimeout(resolve, 100));
                    }

                    if (allAudioBlobs.length === 0) return null;
                    if (allAudioBlobs.length === 1) return allAudioBlobs[0];

                    // åˆå¹¶å¤šä¸ªéŸ³é¢‘ç‰‡æ®µ
                    console.warn('[TTS Download] Merging', allAudioBlobs.length, 'audio segments...');
                    return await mergeWavBlobs(allAudioBlobs);
                },
                getAllAudioBlobs() {
                    // è¿”å›æ‰€æœ‰éŸ³é¢‘ blobs
                    return allAudioBlobs;
                }
            };

            return controller;
        } catch (error) {
            console.error('Error generating speech:', error);
            throw error;
        }
    }

    setVoice(voice) {
        this.voice = voice;
    }

    getAvailableVoices() {
        // è¿”å›æ‰€æœ‰ 54 ä¸ªè¯­éŸ³
        return VOICES.map(v => v.id);
    }
}